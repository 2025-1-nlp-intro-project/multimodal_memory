#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Visual Dialogue ìƒì„± ê²°ê³¼ í‰ê°€ë¥¼ ìœ„í•œ BERTScore ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ Visual Dialogue ìƒì„± ê²°ê³¼ë¥¼ BERTScoreë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í¬ê´„ì ì¸ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
BERTScoreëŠ” BERTì˜ ì‚¬ì „ í›ˆë ¨ëœ ì»¨í…ìŠ¤ì¶”ì–¼ ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ”
í˜ì‹ ì ì¸ í‰ê°€ ì§€í‘œë¡œ, ê¸°ì¡´ì˜ n-gram ê¸°ë°˜ ë©”íŠ¸ë¦­(BLEU, ROUGE)ë³´ë‹¤ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë” ì˜ í¬ì°©í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- BERTScoreë¥¼ ì£¼ìš” ë©”íŠ¸ë¦­ìœ¼ë¡œ í•œ í‰ê°€ (Precision, Recall, F1)
- ROUGE, BLEUë¥¼ ë³´ì¡° ë©”íŠ¸ë¦­ìœ¼ë¡œ ì œê³µ
- ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„ ë° í’ˆì§ˆ ë¶„í¬ ì œê³µ
- íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ ë° ëª¨ë¸ ìºì‹±
- ì§ê´€ì ì¸ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤

ì‚¬ìš©ë²•:
    python bert_evaluate.py --generated predictions.json --ground_truth visdial_1.0_val.json

ì‘ì„±ì: Visual Dialogue í”„ë¡œì íŠ¸ íŒ€
ë¼ì´ì„¼ìŠ¤: MIT
"""

import json
import re
import logging
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass

import torch
import numpy as np
from tqdm import tqdm
from bert_score import score as bert_score_function, BERTScorer
from rouge_score import rouge_scorer
from sacrebleu import corpus_bleu


@dataclass
class EvaluationConfig:
    """í‰ê°€ ì„¤ì •ì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤"""
    bert_model: str = "bert-base-uncased"  # BERT ëª¨ë¸ íƒ€ì…
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    lang: str = "en"  # ì–¸ì–´ ì„¤ì •
    rescale_with_baseline: bool = True  # ë² ì´ìŠ¤ë¼ì¸ ì¬ìŠ¤ì¼€ì¼ë§ ì ìš©
    idf_weighting: bool = True  # IDF ê°€ì¤‘ì¹˜ ì ìš©
    batch_size: int = 64  # ë°°ì¹˜ í¬ê¸°
    max_length: int = 512  # ìµœëŒ€ í† í° ê¸¸ì´
    verbose: bool = True  # ìƒì„¸ ì¶œë ¥
    cache_dir: Optional[str] = None  # BERT ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
    use_fast_tokenizer: bool = True  # ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©


class VisualDialogEvaluator:
    """
    Visual Dialogue í‰ê°€ë¥¼ ìœ„í•œ BERTScore ì¤‘ì‹¬ í‰ê°€ ì‹œìŠ¤í…œ

    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë©”íŠ¸ë¦­ì„ ì œê³µí•©ë‹ˆë‹¤:
    1. BERTScore (Precision, Recall, F1) - ì£¼ìš” ë©”íŠ¸ë¦­
    2. ROUGE (ë³´ì¡° ë©”íŠ¸ë¦­)
    3. BLEU (ë³´ì¡° ë©”íŠ¸ë¦­)  
    """

    def __init__(self, config: Optional[EvaluationConfig] = None):
        """
        í‰ê°€ì ì´ˆê¸°í™”

        Args:
            config: í‰ê°€ ì„¤ì • (Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.config = config or EvaluationConfig()
        self.logger = self._setup_logger()

        # BERTScorer ì´ˆê¸°í™” (ìºì‹±ì„ ìœ„í•´)
        self.logger.info(f"BERTScorer ì´ˆê¸°í™” ({self.config.bert_model}, {self.config.device})")
        try:
            self.bert_scorer = BERTScorer(
                model_type=self.config.bert_model,
                lang=self.config.lang,
                rescale_with_baseline=self.config.rescale_with_baseline,
                idf=self.config.idf_weighting,
                device=self.config.device,
                batch_size=self.config.batch_size,
                nthreads=4,  # ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
                use_fast_tokenizer=self.config.use_fast_tokenizer
            )
            self.logger.info("BERTScorer ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"BERTScorer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # ROUGE ìŠ¤ì½”ì–´ëŸ¬ ì´ˆê¸°í™”
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=True
        )

        self.logger.info(f"Visual Dialog Evaluator ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"BERT Model: {self.config.bert_model}")
        self.logger.info(f"Device: {self.config.device}")
        self.logger.info(f"Language: {self.config.lang}")

    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("VisualDialogEvaluator")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _clean_response(self, response: str) -> str:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜

        Args:
            response: ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            ì •ë¦¬ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        # 1. reasoning íƒœê·¸ ì œê±°
        reasoning_pattern = re.compile(r'<reasoning>.*?</reasoning>', re.DOTALL | re.IGNORECASE)
        cleaned = reasoning_pattern.sub('', response).strip()

        # 2. ê¸°íƒ€ XML íƒœê·¸ ì œê±°
        tag_pattern = re.compile(r'<.*?>', re.DOTALL)
        cleaned = tag_pattern.sub('', cleaned).strip()

        # 3. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        cleaned = ' '.join(cleaned.split())

        return cleaned

    def load_generated_answers(self, generated_path: str, max_samples: Optional[int] = None) -> Dict[int, str]:
        """
        ìƒì„±ëœ ë‹µë³€ ë¡œë“œ ë° ì „ì²˜ë¦¬

        Args:
            generated_path: ìƒì„±ëœ ë‹µë³€ JSON íŒŒì¼ ê²½ë¡œ
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì¸ ê²½ìš° ì „ì²´)

        Returns:
            image_id -> cleaned_response ë§¤í•‘
        """
        self.logger.info(f"ìƒì„±ëœ ë‹µë³€ ë¡œë“œ ì¤‘: {generated_path}")

        if not os.path.exists(generated_path):
            raise FileNotFoundError(f"ìƒì„±ëœ ë‹µë³€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {generated_path}")

        try:
            with open(generated_path, 'r', encoding='utf-8') as f:
                gen_data = json.load(f)
        except json.JSONDecodeError:
            self.logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {generated_path}")
            raise

        if max_samples:
            gen_data = gen_data[:max_samples]

        gen_answers = {}

        for entry in tqdm(gen_data, desc="ìƒì„±ëœ ë‹µë³€ ì²˜ë¦¬"):
            img_id = entry.get("image_id")
            if img_id is None:
                continue

            response = entry.get("response", "").strip()

            # ì‘ë‹µ ì •ë¦¬
            cleaned_response = self._clean_response(response)

            if cleaned_response:
                gen_answers[img_id] = cleaned_response

        self.logger.info(f"ì´ {len(gen_answers)}ê°œì˜ ìƒì„±ëœ ë‹µë³€ ë¡œë“œ ì™„ë£Œ")
        return gen_answers

    def load_ground_truth_answers(self, raw_path: str) -> Dict[int, str]:
        """
        VisDial ë°ì´í„°ì…‹ì—ì„œ ì •ë‹µ ë¡œë“œ

        Args:
            raw_path: VisDial ì›ë³¸ JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            image_id -> ground_truth_answer ë§¤í•‘
        """
        self.logger.info(f"ì •ë‹µ ë°ì´í„° ë¡œë“œ ì¤‘: {raw_path}")

        if not os.path.exists(raw_path):
            raise FileNotFoundError(f"ì •ë‹µ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_path}")

        try:
            with open(raw_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except json.JSONDecodeError:
            self.logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {raw_path}")
            raise

        questions = raw_data["data"]["questions"]
        answers = raw_data["data"]["answers"]

        gt_answers = {}

        for dialog in tqdm(raw_data["data"]["dialogs"], desc="ì •ë‹µ ì²˜ë¦¬"):
            img_id = dialog["image_id"]

            if not dialog["dialog"]:
                continue

            last_turn = dialog["dialog"][-1]
            gt_text = ""

            # ì§ì ‘ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°
            if "answer" in last_turn:
                answer_id = last_turn["answer"]
                if 0 <= answer_id < len(answers):
                    gt_text = answers[answer_id]

            # ë‹µë³€ ì˜µì…˜ì—ì„œ ì„ íƒí•˜ëŠ” ê²½ìš°
            elif "answer_options" in last_turn and "gt_index" in last_turn:
                opt_list = last_turn["answer_options"]
                gt_idx = last_turn["gt_index"]
                if 0 <= gt_idx < len(opt_list) and 0 <= opt_list[gt_idx] < len(answers):
                    gt_text = answers[opt_list[gt_idx]]

            # ì •ë¦¬ëœ ì •ë‹µ ì €ì¥
            if gt_text.strip():
                gt_answers[img_id] = self._clean_response(gt_text)

        self.logger.info(f"ì´ {len(gt_answers)}ê°œì˜ ì •ë‹µ ë¡œë“œ ì™„ë£Œ")
        return gt_answers

    def compute_bertscore(self, candidates: List[str], references: List[str]) -> Dict[str, float]:
        """
        BERTScore ê³„ì‚° (ì£¼ìš” ë©”íŠ¸ë¦­)

        Args:
            candidates: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            references: ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            BERTScore ê²°ê³¼ (precision, recall, f1)
        """
        self.logger.info("BERTScore ê³„ì‚° ì¤‘...")

        try:
            # BERTScorer ì‚¬ìš© (ìºì‹±ëœ ëª¨ë¸)
            P, R, F1 = bert_score_function(candidates, [r[0] for r in references],
                                      lang="en", device='cuda', rescale_with_baseline=False)

            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜ í›„ í‰ê·  ê³„ì‚°
            precision = P.mean().item()
            recall = R.mean().item()
            f1 = F1.mean().item()

            self.logger.info(f"BERTScore ê³„ì‚° ì™„ë£Œ: P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}")

            return {
                "bert_precision": precision,
                "bert_recall": recall,
                "bert_f1": f1,
                "bert_f1_std": F1.std().item(),  # í‘œì¤€í¸ì°¨ ì¶”ê°€
            }
        except Exception as e:
            self.logger.error(f"BERTScore ê³„ì‚° ì˜¤ë¥˜: {e}")
            raise

    def compute_auxiliary_metrics(self, candidates: List[str], references: List[str]) -> Dict[str, float]:
        """
        ë³´ì¡° ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE, BLEU)

        Args:
            candidates: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            references: ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³´ì¡° ë©”íŠ¸ë¦­ ê²°ê³¼
        """
        self.logger.info("ë³´ì¡° ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")

        # ROUGE ì ìˆ˜ ëˆ„ì 
        rouge_scores = {
            "rouge1": {"precision": 0.0, "recall": 0.0, "fmeasure": 0.0},
            "rouge2": {"precision": 0.0, "recall": 0.0, "fmeasure": 0.0},
            "rougeL": {"precision": 0.0, "recall": 0.0, "fmeasure": 0.0}
        }

        total_pairs = len(candidates)

        # ê° ìŒì— ëŒ€í•´ ROUGE ê³„ì‚°
        for cand, ref in tqdm(zip(candidates, references), 
                             total=total_pairs, desc="ROUGE ê³„ì‚°"):
            try:
                # ROUGE ê³„ì‚°
                rouge_result = self.rouge_scorer.score(ref, cand)
                for metric in rouge_scores:
                    for sub_metric in rouge_scores[metric]:
                        rouge_scores[metric][sub_metric] += getattr(rouge_result[metric], sub_metric)
            except Exception as e:
                self.logger.warning(f"ì¼ë¶€ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœ€: {e}")
                continue

        # ROUGE í‰ê·  ê³„ì‚°
        for metric in rouge_scores:
            for sub_metric in rouge_scores[metric]:
                if total_pairs > 0:
                    rouge_scores[metric][sub_metric] /= total_pairs
                else:
                    rouge_scores[metric][sub_metric] = 0.0

        # BLEU ì ìˆ˜ ê³„ì‚° (corpus-level)
        try:
            bleu_result = corpus_bleu(candidates, [[ref] for ref in references])
            bleu_score = bleu_result.score / 100.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        except Exception as e:
            self.logger.warning(f"BLEU ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            bleu_score = 0.0

        self.logger.info(f"ë³´ì¡° ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
        self.logger.info(f"ROUGE-1 F1: {rouge_scores['rouge1']['fmeasure']:.4f}")
        self.logger.info(f"ROUGE-L F1: {rouge_scores['rougeL']['fmeasure']:.4f}")
        self.logger.info(f"BLEU: {bleu_score:.4f}")

        return {
            "rouge1_f1": rouge_scores["rouge1"]["fmeasure"],
            "rouge2_f1": rouge_scores["rouge2"]["fmeasure"],
            "rougeL_f1": rouge_scores["rougeL"]["fmeasure"],
            "bleu": bleu_score,
            "total_samples": total_pairs
        }

    def compute_detailed_bertscore_analysis(self, candidates: List[str], references: List[str]) -> Dict[str, Any]:
        """
        BERTScore ìƒì„¸ ë¶„ì„ ìˆ˜í–‰

        Args:
            candidates: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            references: ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("BERTScore ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

        try:
            P, R, F1 = self.bert_scorer.score(candidates, references)

            # ê°œë³„ ì ìˆ˜ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            p_scores = P.cpu().numpy()
            r_scores = R.cpu().numpy()
            f1_scores = F1.cpu().numpy()

            # ë¶„í¬ ë¶„ì„
            analysis = {
                "score_distribution": {
                    "precision": {
                        "mean": float(np.mean(p_scores)),
                        "std": float(np.std(p_scores)),
                        "min": float(np.min(p_scores)),
                        "max": float(np.max(p_scores)),
                        "median": float(np.median(p_scores)),
                        "q25": float(np.percentile(p_scores, 25)),
                        "q75": float(np.percentile(p_scores, 75))
                    },
                    "recall": {
                        "mean": float(np.mean(r_scores)),
                        "std": float(np.std(r_scores)),
                        "min": float(np.min(r_scores)),
                        "max": float(np.max(r_scores)),
                        "median": float(np.median(r_scores)),
                        "q25": float(np.percentile(r_scores, 25)),
                        "q75": float(np.percentile(r_scores, 75))
                    },
                    "f1": {
                        "mean": float(np.mean(f1_scores)),
                        "std": float(np.std(f1_scores)),
                        "min": float(np.min(f1_scores)),
                        "max": float(np.max(f1_scores)),
                        "median": float(np.median(f1_scores)),
                        "q25": float(np.percentile(f1_scores, 25)),
                        "q75": float(np.percentile(f1_scores, 75))
                    }
                },
                "performance_brackets": {
                    "high_quality": int(np.sum(f1_scores >= 0.8)),
                    "medium_quality": int(np.sum((f1_scores >= 0.6) & (f1_scores < 0.8))),
                    "low_quality": int(np.sum(f1_scores < 0.6))
                }
            }

            # ìµœê³ /ìµœì € ì ìˆ˜ ìƒ˜í”Œ ì°¾ê¸° (ì¸ë±ìŠ¤ ë° ì ìˆ˜)
            top_idx = np.argmax(f1_scores)
            bottom_idx = np.argmin(f1_scores)

            analysis["examples"] = {
                "highest_score": {
                    "f1": float(f1_scores[top_idx]),
                    "candidate": candidates[top_idx],
                    "reference": references[top_idx]
                },
                "lowest_score": {
                    "f1": float(f1_scores[bottom_idx]),
                    "candidate": candidates[bottom_idx],
                    "reference": references[bottom_idx]
                }
            }

            return analysis
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ìµœì†Œí•œì˜ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return {
                "score_distribution": {},
                "performance_brackets": {
                    "high_quality": 0,
                    "medium_quality": 0,
                    "low_quality": 0
                },
                "examples": {}
            }

    def evaluate(self, generated_path: str, ground_truth_path: str, 
                max_samples: Optional[int] = None, 
                output_path: Optional[str] = None) -> Dict[str, Any]:
        """
        ì „ì²´ í‰ê°€ ìˆ˜í–‰

        Args:
            generated_path: ìƒì„±ëœ ë‹µë³€ íŒŒì¼ ê²½ë¡œ
            ground_truth_path: ì •ë‹µ íŒŒì¼ ê²½ë¡œ
            max_samples: ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Noneì¸ ê²½ìš° ì €ì¥í•˜ì§€ ì•ŠìŒ)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("=== Visual Dialogue BERTScore í‰ê°€ ì‹œì‘ ===")

        # ë°ì´í„° ë¡œë“œ
        gen_answers = self.load_generated_answers(generated_path, max_samples)
        gt_answers = self.load_ground_truth_answers(ground_truth_path)

        # ê³µí†µ ì´ë¯¸ì§€ ID ì°¾ê¸°
        common_ids = set(gen_answers.keys()) & set(gt_answers.keys())
        self.logger.info(f"í‰ê°€ ê°€ëŠ¥í•œ ê³µí†µ ìƒ˜í”Œ ìˆ˜: {len(common_ids)}")

        if not common_ids:
            raise ValueError("í‰ê°€ ê°€ëŠ¥í•œ ê³µí†µ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        sorted_ids = sorted(common_ids)
        candidates = [gen_answers[img_id] for img_id in sorted_ids]
        references = [gt_answers[img_id] for img_id in sorted_ids]

        # BERTScore ê³„ì‚° (ì£¼ìš” ë©”íŠ¸ë¦­)
        bert_results = self.compute_bertscore(candidates, references)

        # ë³´ì¡° ë©”íŠ¸ë¦­ ê³„ì‚°
        auxiliary_results = self.compute_auxiliary_metrics(candidates, references)

        # BERTScore ìƒì„¸ ë¶„ì„
        detailed_analysis = self.compute_detailed_bertscore_analysis(candidates, references)

        # ê²°ê³¼ í†µí•©
        evaluation_results = {
            "evaluation_info": {
                "generated_file": generated_path,
                "ground_truth_file": ground_truth_path,
                "total_samples": len(common_ids),
                "bert_model": self.config.bert_model,
                "evaluation_config": {
                    "rescale_with_baseline": self.config.rescale_with_baseline,
                    "idf_weighting": self.config.idf_weighting,
                    "language": self.config.lang
                }
            },
            "primary_metrics": {
                "bertscore": bert_results
            },
            "auxiliary_metrics": auxiliary_results,
            "detailed_analysis": detailed_analysis,
            "sample_comparisons": self._generate_sample_comparisons(
                candidates[:5], references[:5], sorted_ids[:5]
            )
        }

        # ê²°ê³¼ ì €ì¥
        if output_path:
            self._save_results(evaluation_results, output_path)

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_summary(evaluation_results)

        return evaluation_results

    def _generate_sample_comparisons(self, candidates: List[str], references: List[str], 
                                   image_ids: List[int]) -> List[Dict]:
        """ìƒ˜í”Œ ë¹„êµ ê²°ê³¼ ìƒì„±"""
        comparisons = []

        # ê°œë³„ BERTScore ê³„ì‚°
        P, R, F1 = bert_score_function(candidates, [r[0] for r in references],
                                      lang="en", device='cuda', rescale_with_baseline=False)

        for i, (cand, ref, img_id) in enumerate(zip(candidates, references, image_ids)):
            comparisons.append({
                "image_id": img_id,
                "candidate": cand,
                "reference": ref,
                "bertscore": {
                    "precision": P[i].item(),
                    "recall": R[i].item(),
                    "f1": F1[i].item()
                }
            })

        return comparisons

    def _save_results(self, results: Dict[str, Any], output_path: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        self.logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")

        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            self.logger.info("í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _print_summary(self, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("" + "="*60)
        print("           VISUAL DIALOGUE EVALUATION RESULTS")
        print("="*60)

        # ì£¼ìš” ë©”íŠ¸ë¦­ (BERTScore)
        bert_scores = results["primary_metrics"]["bertscore"]
        print(f"ğŸ¯ PRIMARY METRICS (BERTScore)")
        print(f"   Precision: {bert_scores['bert_precision']:.4f}")
        print(f"   Recall:    {bert_scores['bert_recall']:.4f}")
        print(f"   F1-Score:  {bert_scores['bert_f1']:.4f} (Â±{bert_scores['bert_f1_std']:.4f})")

        # ë³´ì¡° ë©”íŠ¸ë¦­
        aux_metrics = results["auxiliary_metrics"]
        print(f"ğŸ“Š AUXILIARY METRICS")
        print(f"   ROUGE-1:   {aux_metrics['rouge1_f1']:.4f}")
        print(f"   ROUGE-2:   {aux_metrics['rouge2_f1']:.4f}")
        print(f"   ROUGE-L:   {aux_metrics['rougeL_f1']:.4f}")
        print(f"   BLEU:      {aux_metrics['bleu']:.4f}")

        # í’ˆì§ˆ ë¶„í¬
        brackets = results["detailed_analysis"]["performance_brackets"]
        total = sum(brackets.values())
        if total > 0:
            print(f"ğŸ“ˆ QUALITY DISTRIBUTION")
            print(f"   High (F1â‰¥0.8):   {brackets['high_quality']:4d} ({brackets['high_quality']/total*100:.1f}%)")
            print(f"   Medium (0.6â‰¤F1<0.8): {brackets['medium_quality']:4d} ({brackets['medium_quality']/total*100:.1f}%)")
            print(f"   Low (F1<0.6):    {brackets['low_quality']:4d} ({brackets['low_quality']/total*100:.1f}%)")

        print(f"   Total Samples: {results['evaluation_info']['total_samples']}")
        print("="*60 + "")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="Visual Dialogue BERTScore í‰ê°€")
    parser.add_argument("--generated", "-g", required=True, 
                       help="ìƒì„±ëœ ë‹µë³€ JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--ground_truth", "-t", required=True,
                       help="VisDial ì •ë‹µ JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", "-o", default="evaluation_results.json",
                       help="ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--max_samples", "-m", type=int,
                       help="ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--bert_model", "-b", default="bert-base-uncased",
                       help="ì‚¬ìš©í•  BERT ëª¨ë¸")
    parser.add_argument("--device", "-d", default="auto",
                       help="ì—°ì‚° ë””ë°”ì´ìŠ¤ (cuda/cpu/auto)")
    parser.add_argument("--no_baseline", action="store_true",
                       help="ë² ì´ìŠ¤ë¼ì¸ ì¬ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™”")
    parser.add_argument("--no_idf", action="store_true",
                       help="IDF ê°€ì¤‘ì¹˜ ë¹„í™œì„±í™”")
    parser.add_argument("--batch_size", type=int, default=64,
                       help="ë°°ì¹˜ í¬ê¸°")

    args = parser.parse_args()

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device

    # ì„¤ì • ìƒì„±
    config = EvaluationConfig(
        bert_model=args.bert_model,
        device=device,
        rescale_with_baseline=not args.no_baseline,
        idf_weighting=not args.no_idf,
        batch_size=args.batch_size
    )

    # í‰ê°€ ì‹¤í–‰
    evaluator = VisualDialogEvaluator(config)

    try:
        results = evaluator.evaluate(
            generated_path=args.generated,
            ground_truth_path=args.ground_truth,
            max_samples=args.max_samples,
            output_path=args.output
        )

        print(f"âœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
