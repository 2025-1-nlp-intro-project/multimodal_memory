# <reminder> ë°©ë²•ë¡ 
## Additional by ì—„íƒœìš°

Visual Dialogue ë°ì´í„°ì…‹ì„ í™œìš©í•œ Gemma-3 ëª¨ë¸ íŒŒì¸íŠœë‹ ë° í‰ê°€ ì‹œìŠ¤í…œì— ëŒ€í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œ ììœ ë¡­ê²Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” VisDial ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ ëª¨ë¸ì¸ Gemma-3ë¥¼ ì‹œê°ì  ëŒ€í™”(Visual Dialogue)ì— íŠ¹í™”ì‹œí‚¤ê¸° ìœ„í•œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
reminder_tag/
â”œâ”€â”€ README.md                    # êµìˆ˜ììš© ë©”ì¸ ê°€ì´ë“œ
â”œâ”€â”€ requirements.txt             # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ setup.py                     # íŒ¨í‚¤ì§€ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ data/                        # ë°ì´í„° ê´€ë ¨ íŒŒì¼
â”‚   â”œâ”€â”€ README.md                # ë°ì´í„° ì„¤ëª…ì„œ
â”‚   â””â”€â”€ download_data.py         # ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ src/                         # ì†ŒìŠ¤ ì½”ë“œ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ data_generation/         # ë°ì´í„° ìƒì„± ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gen_base.py          # ê¸°ë³¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â”œâ”€â”€ gen_api.py           # API ê¸°ë°˜ ë°ì´í„° ìƒì„±
â”‚   â”‚   â””â”€â”€ gen_eval.py          # í‰ê°€ ë°ì´í„° ìƒì„±
â”‚   â”œâ”€â”€ training/                # íŒŒì¸íŠœë‹ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ finetune.py          # íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ inference/               # ì¶”ë¡  ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ inference.py         # ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluation/              # í‰ê°€ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluation.py        # ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ utils/                   # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py           # ê³µí†µ í•¨ìˆ˜ ëª¨ìŒ
â”œâ”€â”€ configs/                     # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ training_config.yaml     # í›ˆë ¨ ì„¤ì •
â”‚   â””â”€â”€ evaluation_config.yaml   # í‰ê°€ ì„¤ì •
â”œâ”€â”€ scripts/                     # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ run_pipeline.sh          # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
â”‚   â””â”€â”€ quick_start.sh           # ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ notebooks/                   # êµìœ¡ìš© ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_finetuning.ipynb
â”‚   â””â”€â”€ 03_evaluation.ipynb
â””â”€â”€ tests/                       # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
    â””â”€â”€ test_evaluation.py
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.9+
- CUDA ì§€ì› GPU (ìµœì†Œ 16GB ì´ìƒ VRAM ê¶Œì¥)
- ìµœì†Œ 32GB RAM
- 200GB ì´ìƒì˜ ì €ì¥ ê³µê°„ (ë°ì´í„°ì…‹ ë° ëª¨ë¸ í¬í•¨)

### ë¹ ë¥¸ ì‹œì‘
```bash
bash quick_start.sh
```

### ì„¤ì¹˜ ë°©ë²•

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/2025-1-nlp-intro-project/multimodal_memory.git
cd multimodal_memory/reminder_tag

# 2. ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
conda create -n visdial python=3.10 -y
conda activate visdial

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

í”„ë¡œì íŠ¸ì—ì„œ ì œê³µí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ Visual Dialogue ë°ì´í„°ì…‹ê³¼ COCO ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ
python data/download_data.py

# íŠ¹ì • ë¶„í• ë§Œ ë‹¤ìš´ë¡œë“œ (ì„ íƒì )
python data/download_data.py --split train
python data/download_data.py --split val
```

## ğŸ’¾ ë°ì´í„° ìƒì„±

ëª¨ë¸ í›ˆë ¨ì— í•„ìš”í•œ ë°ì´í„° ìƒì„± ê³¼ì •ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ë°ì´í„° ìƒì„±

```bash
# ê¸°ë³¸ ë°ì´í„° ìƒì„± (Ollama ì‚¬ìš©)
python -m src.data_generation.gen_base \
    --annotation_file data/visdial/annotations/instances_train2014.json \
    --visdial_path data/visdial/data/visdial_1.0_train.json \
    --output_path outputs/data_base.json \
    --max_samples 1000

# ì¶”ë¡  ê³¼ì • í¬í•¨ ë°ì´í„° ìƒì„±
python -m src.data_generation.gen_api \
    --annotation_file data/visdial/annotations/instances_train2014.json \
    --visdial_path data/visdial/data/visdial_1.0_train.json \
    --output_path outputs/data_api.json \
    --export_training outputs/training_data.json \
    --max_samples 1000
```

### í‰ê°€ ë°ì´í„° ìƒì„±

ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±:

```bash
# í‰ê°€ ë°ì´í„° ìƒì„±
python -m src.data_generation.gen_eval \
    --visdial_path data/visdial/data/visdial_1.0_val.json \
    --image_dir data/visdial/images/val2018/ \
    --output_path outputs/eval_data.json \
    --export_predictions outputs/predictions.json \
    --max_samples 500

# í‰ê°€ ì§€í‘œ ê³„ì‚°
python src/evaluation/evaluate.py \
    --generated outputs/predictions.json \
    --ground_truth data/visdial/data/visdial_1.0_val.json \
    --output outputs/evaluation_results.json
```

## ğŸ”§ ëª¨ë¸ íŒŒì¸íŠœë‹

ëª¨ë¸ íŒŒì¸íŠœë‹ì€ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ íŒŒì¸íŠœë‹
python src/training/finetune.py --config configs/training_config.yaml

# ê³ ê¸‰ ì„¤ì •ìœ¼ë¡œ íŒŒì¸íŠœë‹
python src/training/finetune.py \
  --model_name "unsloth/gemma-3-4b-it" \
  --dataset_path data/generated/api_dataset.json \
  --output_dir outputs/gemma3_finetune \
  --batch_size 2 \
  --grad_accum 4 \
  --lr 1e-5 \
  --epochs 3
```

### íŒŒì¸íŠœë‹ ì„¤ì • ì˜µì…˜

`configs/training_config.yaml`ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# ëª¨ë¸ ì„¤ì •
model:
  name: "unsloth/gemma-3-4b-it"
  load_in_4bit: true
  use_gradient_checkpointing: "unsloth"

# PEFT ì„¤ì •
peft:
  finetune_vision_layers: true
  finetune_language_layers: true
  finetune_attention_modules: true
  finetune_mlp_modules: true
  r: 16
  lora_alpha: 16
  lora_dropout: 0
  bias: "none"
  use_rslora: false

# í›ˆë ¨ ì„¤ì •
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_steps: 5
  learning_rate: 1e-5
  fp16: false
  bf16: true
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  max_seq_length: 2048
  output_dir: "outputs"
```

## ğŸ” ì¶”ë¡  ì‹¤í–‰

íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•:

```bash
# ê¸°ë³¸ ì¶”ë¡ 
python src/inference/inference.py \
  --model_path outputs/gemma3_finetune \
  --image_path path/to/image.jpg \
  --conversation "Q: What do you see in this image?\nA: I see children playing on the beach.\nQ: How many children are there?"

# ë°°ì¹˜ ì¶”ë¡ 
python src/inference/inference.py \
  --model_path outputs/gemma3_finetune \
  --batch_file data/test_batch.json \
  --output_file results/batch_results.json
```

## ğŸ“Š í‰ê°€ ì‹¤í–‰

ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:

```bash
# ê¸°ë³¸ í‰ê°€
python src/evaluation/evaluation.py \
  --prediction_file results/batch_results.json \
  --groundtruth_file data/visdial/data/visdial_1.0_val.json

# ìƒì„¸ í‰ê°€ ì§€í‘œ ì¶œë ¥
python src/evaluation/evaluation.py \
  --prediction_file results/batch_results.json \
  --groundtruth_file data/visdial/data/visdial_1.0_val.json \
  --verbose
```

### í‰ê°€ ì§€í‘œ (Evaluation Metrics)

Visual Dialogue íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸ëŠ” **BERTScoreë¥¼ ì£¼ìš” ë©”íŠ¸ë¦­**ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, ë³´ì¡° ë©”íŠ¸ë¦­ë“¤ê³¼ í•¨ê»˜ í¬ê´„ì ì¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**ğŸ¯ ì£¼ìš” ë©”íŠ¸ë¦­: BERTScore**

#### BERTScoreë€?

BERTScoreëŠ” BERTì˜ ì‚¬ì „ í›ˆë ¨ëœ ì»¨í…ìŠ¤ì¶”ì–¼ ì„ë² ë”©ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” í˜ì‹ ì ì¸ í‰ê°€ ì§€í‘œì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ n-gram ê¸°ë°˜ ë©”íŠ¸ë¦­ë“¤(BLEU, ROUGE)ê³¼ ë‹¬ë¦¬, **ë‹¨ìˆœí•œ ë‹¨ì–´ ë§¤ì¹­ì„ ë„˜ì–´ì„œ ì‹¤ì œ ì˜ë¯¸ë¥¼ ì´í•´**í•©ë‹ˆë‹¤.

#### ì™œ BERTScoreì¸ê°€?

ğŸ” ê¸°ì¡´ ë©”íŠ¸ë¦­ì˜ í•œê³„
```
ì°¸ì¡° ë‹µë³€: "The food was delicious."
ìƒì„± ë‹µë³€: "I loved the meal."
```

- **BLEU/ROUGE**: ë‹¨ì–´ê°€ ë‹¬ë¼ì„œ ë‚®ì€ ì ìˆ˜ (0.0)
- **BERTScore**: ì˜ë¯¸ê°€ ìœ ì‚¬í•˜ë¯€ë¡œ ë†’ì€ ì ìˆ˜ (0.85+)

âœ¨ BERTScoreì˜ ì¥ì 

1. **ì˜ë¯¸ì  ì´í•´**: ë™ì˜ì–´, íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹
2. **ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤**: ë¬¸ì¥ ë‚´ ë‹¨ì–´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ íŒŒì•…
3. **ìœ ì—°í•œ í‰ê°€**: ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì„ ê³µì •í•˜ê²Œ í‰ê°€
4. **ì¸ê°„ í‰ê°€ì™€ ë†’ì€ ìƒê´€ê´€ê³„**: ì‹¤ì œ ì¸ê°„ì˜ íŒë‹¨ê³¼ ë§¤ìš° ìœ ì‚¬

## ğŸ§ª ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

ë‹¨ì¼ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸(ë°ì´í„° ìƒì„±, í›ˆë ¨, í‰ê°€)ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
./scripts/run_pipeline.sh

# ê³ ê¸‰ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
./scripts/run_pipeline.sh --config configs/custom_config.yaml --output experiment_1
```

# (2024CVPR) MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding
### [Project Page](https://boheumd.github.io/MA-LMM/) | [Paper](https://arxiv.org/abs/2404.05726)
The official repository of our paper "**MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding**".

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ma-lmm-memory-augmented-large-multimodal/video-classification-on-breakfast)](https://paperswithcode.com/sota/video-classification-on-breakfast?p=ma-lmm-memory-augmented-large-multimodal)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ma-lmm-memory-augmented-large-multimodal/video-classification-on-coin-1)](https://paperswithcode.com/sota/video-classification-on-coin-1?p=ma-lmm-memory-augmented-large-multimodal)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ma-lmm-memory-augmented-large-multimodal/visual-question-answering-on-msvd-qa-1)](https://paperswithcode.com/sota/visual-question-answering-on-msvd-qa-1?p=ma-lmm-memory-augmented-large-multimodal)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ma-lmm-memory-augmented-large-multimodal/video-question-answering-on-msrvtt-qa)](https://paperswithcode.com/sota/video-question-answering-on-msrvtt-qa?p=ma-lmm-memory-augmented-large-multimodal)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ma-lmm-memory-augmented-large-multimodal/video-captioning-on-youcook2)](https://paperswithcode.com/sota/video-captioning-on-youcook2?p=ma-lmm-memory-augmented-large-multimodal)


<p align="center">
<img src="EMR/figs/teaser.png" alt="teaser" width="60%">
</p>


## Model Overview
<p align="center">
<img src="EMR/figs/architecture.png" alt="model" width="80%">
</p>

## Demo
You can explore our demo by running `demo.ipynb`. This demonstration illustrates how our MA-LMM serves as a plug-and-play module that can be integrated into InstructBLIP seamlessly, requiring no fine-tuning for zero-shot evaluation.

## Requirements

You can install the conda environment by running:
```bash
git clone https://github.com/boheumd/MA-LMM.git
cd MA-LMM
pip install -e .
```

If you are running the code on Apple Silicon, you need to use `eva-decord` instead of `decord`. Here is the modification in the `requirements.txt` file you should do:

```text
contexttimer
eva-decord
einops>=0.4.1
fairscale==0.4.4
...
```

**Before running `pip install -e .`, ensure you have the correct requirements.**

## Dataset
For the long-term video understanding task, we conduct experiments including ([LVU](https://github.com/chaoyuaw/lvu)) and two standard video summarization datasets ([Breakfast](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/), [COIN](https://coin-dataset.github.io/)).

For the video question answering task, we conduct experiments including [MSRVTT](https://github.com/xudejing/video-question-answering), [MSVD](https://github.com/xudejing/video-question-answering), and [ActivityNet](https://github.com/MILVLG/activitynet-qa).
For the video captioning task, we also conduct experiments on [Youcook2](http://youcook2.eecs.umich.edu/) dataset.

You can download videos for each dataset through the script provided here (lavis/datasets/download_scripts). For LVU/Breakfast/COIN datasets, please download the original videos through the official link provided above.

Then extract video frames of each video with fps=10. Example preprocess code is provided here [extract_frames.py](https://github.com/boheumd/MA-LMM/blob/main/data/extract_frames.py).
Since different FFMPEG versions are used, the actual extracted frame lengths can be slightly inconsistent. You may need to update the actual frame_length for each video in the annotation file.
   ```
    â”œâ”€â”€ data
        â””â”€â”€ activitynet
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ breakfast
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ coin
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ lvu
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ msrvtt
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ msvd
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
        â””â”€â”€ youcook2
            â”œâ”€â”€ annotation
            â”œâ”€â”€ frames
            â”œâ”€â”€ videos
   ```



## Running

### Download Pre-trained LLM
We use Vicuna-v1.1 as our pre-trained LLM weights, you can download from this [link](https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md) as arrange in this format.
   ```
   â”œâ”€â”€ llm
        â”œâ”€â”€ vicuna-7b
        â”œâ”€â”€ vicuna-13b
   ```
### Finetuning on Downstreaming Tasks
Our model leverages pre-trained weights from [InstructBlip](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip), which was only pre-trained on image-text pairs. Our training process occurred on four A100 GPUs. If you would like to fine-tune the model for various video datasets, please run the following command:
```bash
bash run_scripts/${dataset}/train.sh
```

#### LVU dataset
```bash
    # Please choose the task from the following list
    # ['director', 'genre', 'relationship', 'scene', 'way_speaking', 'writer', 'year']
    datasets.lvu_cls.task ${task}
```

### Testing
We also provided finetuned checkpoints for each video dataset. Please download the [saved_model.tar](https://drive.google.com/file/d/1mq6fg69Ofm32-1HjEunoFtPg8ymAIcOp/view?usp=sharing) and unzip it. 
For the test script corresponding to each dataset, provide the path to the extracted checkpoint to execute the evaluation.
```bash
bash run_scripts/${dataset}/test.sh ${checkpoint_path}
```

### Zero-shot Evaluation
Our model can also leverage pre-trained weights from [InstructBlip](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) without any finetuning to conduct zero-shot evaluation on video datasets.
```bash
bash run_scripts/${dataset}/test.sh
```


### Hyper-parameters
One important hyper-parameters memory_bank_length, please change that in the training script on different datasets.
```bash
    # pre-defined length of the memory bank
    model.memory_bank_length ${value}
    # value=0 means without using the memory bank
```

### Memory Bank Compression Code
The core algorithm for the memory bank compression algorithm is [here](https://github.com/boheumd/MA-LMM/blob/main/lavis/models/blip2_models/blip2.py#L352).

## Citation
If you find our code or our paper useful for your research, please **[â˜…star]** this repo and **[cite]** the following paper:

```latex
@inproceedings{he2024malmm,
  title = {MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding},
  author    = {He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024}
}
```


## Acknowledgement
We referenced the repo below for the code
- [LAVIS](https://github.com/salesforce/LAVIS)



## Additional by ì´ê°•ìš±

### MA-LMM with ImageBind: Multimodal Memory & Dialogue System

#### 1. í”„ë¡œì íŠ¸ ê°œìš” (Overview)
ë³¸ í”„ë¡œì íŠ¸ëŠ” Vision, Audio ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í•˜ë‚˜ì˜ ì„ë² ë”© ê³µê°„ì— ê²°í•©í•˜ëŠ” ImageBind ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ì¥ê¸° ê¸°ì–µ ë° ì¶”ë¡  ëŠ¥ë ¥ì„ ê°–ì¶˜ ë©€í‹°ëª¨ë‹¬ ê¸°ì–µ ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  
ê¸°ì¡´ ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM)ì˜ í•œê³„ì¸ ì¥ê¸° ê¸°ì–µ ë¶€ì¡±ê³¼ ì¶”ë¡  ì˜¤ë¥˜ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, **MA-LMM (Memory-Augmented Large Multimodal Model)**ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¨ìš©í•˜ê³ , ì—¬ê¸°ì— ImageBindì˜ ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”© ëŠ¥ë ¥ì„ ê²°í•©í–ˆìŠµë‹ˆë‹¤.

#### 2. ìµœì¢… ëª¨ë¸ ì•„í‚¤í…ì²˜
ë³¸ í”„ë¡œì íŠ¸ì—ì„œ êµ¬í˜„ëœ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë…ìì ì¸ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

##### ì…ë ¥ ì¸ì½”ë” (Frozen)
- **Image Encoder**: ImageBind-Hugeì˜ Vision Encoder (ì¶œë ¥: 1024ì°¨ì›)  
- **Text Encoder**: ImageBind-Hugeì˜ Text Encoder (ì¶œë ¥: 1024ì°¨ì›)

##### ì—°ê²° ëª¨ë“ˆ (Trainable)
- **FC Layer (imagebind_fc)**: ImageBindì˜ Vision, Text ì„ë² ë”©ì„ ê²°í•©í•œ 2048ì°¨ì› ë²¡í„°ë¥¼ ë°›ì•„, Q-Formerê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” 1024ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” `nn.Linear(2048, 1024)` ë ˆì´ì–´  
- **ìˆœì„œ ì„ë² ë”© (turn_pe)**: Visual Dialogì˜ í„´(turn) ìˆœì„œ ì •ë³´(0~10)ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ `nn.Embedding(11, 1024)` ë ˆì´ì–´

##### ë¸Œë¦¿ì§€ ëª¨ë“ˆ (Frozen)
- **Q-Former**: Vision íŠ¹ì§•ê³¼ ì–¸ì–´(ì§ˆë¬¸)ë¥¼ ì—°ê²°í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ. ì…ë ¥ìœ¼ë¡œ 1024ì°¨ì›ì˜ íŠ¹ì§• ë²¡í„°ë¥¼ ë°›ë„ë¡ hidden_size=1024ë¡œ ì„¤ì •  
- **ì–¸ì–´ ìƒì„± ëª¨ë“ˆ (Frozen)**  
  - **Projection Layer (llm_proj)**: Q-Formerì˜ ì¶œë ¥(1024ì°¨ì›)ì„ LLMì˜ ì…ë ¥ ì°¨ì›(2048ì°¨ì›)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” `nn.Linear(1024, 2048)` ë ˆì´ì–´  
  - **LLM**: Llama-3.2-1B ëª¨ë¸ì„ bitsandbytesë¥¼ í†µí•´ 4-bit ì–‘ìí™”í•˜ì—¬ ë¡œë“œ

#### 3. í•™ìŠµ íŒŒì´í”„ë¼ì¸
ImageBindì™€ MA-LMMì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ì¶©ëŒ(PyTorch, timm ë²„ì „ ë“±)ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ê°ìì˜ ì—­í• ì„ ë…ë¦½ëœ ê°€ìƒí™˜ê²½ì—ì„œ ìˆ˜í–‰í•˜ëŠ” 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ë°©ì‹ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

##### 1ë‹¨ê³„: ì„ë² ë”© ì‚¬ì „ ì¶”ì¶œ (in imagebind_env)
ImageBind ì „ìš© ê°€ìƒí™˜ê²½ì—ì„œ, VisDial v1.0 ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì´ë¯¸ì§€ì™€ ê° ëŒ€í™” í„´ì˜ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì— ëŒ€í•œ Vision/Text ì„ë² ë”©ì„ ë¯¸ë¦¬ ì¶”ì¶œí•˜ì—¬ ë””ìŠ¤í¬ì— `.pt` íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

##### 2ë‹¨ê³„: ëª¨ë¸ íŒŒì¸íŠœë‹ (in malmm env)
MA-LMM í•™ìŠµìš© ê°€ìƒí™˜ê²½ì—ì„œ, ì‚¬ì „ ì¶”ì¶œëœ ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì˜¤ì§ `imagebind_fc`ì™€ `turn_pe` ë ˆì´ì–´ë§Œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

#### 4. ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ

##### 4.1 í™˜ê²½ ì„¤ì •

###### imagebind_env ìƒì„± (ì„ë² ë”© ì¶”ì¶œìš©)
```bash
conda env create -f environment_imagebind.yml
conda activate imagebind_env
```

###### malmm_env ìƒì„± (ëª¨ë¸ íŒŒì¸íŠœë‹ìš©)

```bash
conda env create -f environment_malmm.yml
conda activate malmm
```

ì°¸ê³ : environment_*.yml íŒŒì¼ì€ conda env export > [íŒŒì¼ì´ë¦„].yml ëª…ë ¹ì–´ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### 4.2 ë°ì´í„° ì¤€ë¹„ ë° ì‹¤í–‰

â€¢	ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
lavis/datasets/visdial/ í´ë”ì— VisDial v1.0 ì–´ë…¸í…Œì´ì…˜ê³¼ MS COCO 2014, VisualDialog_val2018 ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•œ í›„ ì••ì¶• í•´ì œ
â€¢	ì„ë² ë”© ì¶”ì¶œ
```bash
conda activate imagebind_env
python extract_visdial_embeddings.py
```
â€¢	ëª¨ë¸ íŒŒì¸íŠœë‹
```bash
conda activate malmm
cd MA-LMM
python -m lavis.tasks.run â€“cfg-path lavis/configs/tasks/finetune_visdial.yaml
```
â€¢	ì„±ëŠ¥ í‰ê°€
```bash
python evaluate.py \
â€“checkpoint â€œlavis/output/finetune_visdial/finetune_visdial/checkpoint_latest.pthâ€ \
â€“llm_model_path â€œllm/llama-3.2-1B/Llama-3.2-1Bâ€
```
-- checkpoint ì—, ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ ì ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì ì ˆíˆ ì§€ì •í•©ë‹ˆë‹¤. 
! ì£¼ì˜í•  ì ì€, cc3m datasetì˜ ê²½ìš° lavis/models/../blip2.pyì˜, 208 lineì˜ 
```bash
encoder_config.hidden_size = 1024 # 768 -> íŒŒì¸íŠœë‹ ì´ì „ ê²°ê³¼ë¥¼ ìœ„í•´ì„œ, 768ìœ¼ë¡œ ëŒë ¤ì•¼ í•¨.
```

#### 5. ì‹¤í—˜ ê²°ê³¼

ëª¨ë¸	BLEU-4 Score
íŒŒì¸íŠœë‹ ì „ (FC Layerë§Œ í•™ìŠµ)	14.96
íŒŒì¸íŠœë‹ í›„ (VisDialë¡œ FC+PE í•™ìŠµ)	24.80

ë¶„ì„: VisDial ë°ì´í„°ì…‹ê³¼ ìˆœì„œ ì„ë² ë”©ì„ ì´ìš©í•œ íŒŒì¸íŠœë‹ì„ í†µí•´ BLEU-4 ì ìˆ˜ê°€ ì•½ 65.8% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

#### 6. í–¥í›„ ê³¼ì œ (Future Work)
  â€¢	Retrieval ë¡œì§ êµ¬í˜„: Recency, Frequency, Saliency ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì—¬ ëŒ€í™” ë§¥ë½ì— ë§ëŠ” ê³¼ê±° ê¸°ì–µì„ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ëª¨ë“ˆ ì¶”ê°€
  â€¢	Self-Reflection ì ìš©: ê²€ìƒ‰ëœ ê¸°ì–µì„ <reminder> íƒœê·¸ë¡œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•˜ê³ , ëª¨ë¸ì´ ì—°ì‡„ì  ì‚¬ê³ (CoT)ë¥¼ í†µí•´ ë” ë…¼ë¦¬ì ì¸ ë‹µë³€ ìƒì„±
  â€¢	ì¢…í•© í‰ê°€: VisDialë¿ë§Œ ì•„ë‹ˆë¼ ScienceQA ë“± ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ì •ëŸ‰Â·ì •ì„± í‰ê°€ ìˆ˜í–‰